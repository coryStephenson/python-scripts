# Import statements: Third-party libraries
import numpy as np
import pandas as pd
import joblib

# Import statements: Local modules
from sklearn.preprocessing import LabelEncoder
from hmmlearn import hmm

# Load datasets
data1 = pd.read_csv('IDAN1.csv')
data2 = pd.read_csv('IDAN2.csv')
data3 = pd.read_csv('IDAN3.csv')

# Check columns in each dataset
print("Columns in data1:", data1.columns)
print("Columns in data2:", data2.columns)
print("Columns in data3:", data3.columns)

'''
The code on lines 20 through 53 extracts opcode sequences from multiple datasets and converts 
each sequence into a list of individual opcodes. It then combines all 
sequences into one list and uses a LabelEncoder to convert the opcodes 
into numerical values for machine learning. Finally, it encodes each 
sequence into its numeric form, displaying a sample of the encoded 
sequences for verification.
'''

# Extract opcode sequences from each dataset
opcode_sequences1 = data1['Opcode'].astype(str)
opcode_sequences2 = data2['Opcode'].astype(str)
opcode_sequences3 = data3['Opcode'].astype(str)

# Convert each sequence into a list of opcodes
opcode_sequences1 = [sequence.split() for sequence in opcode_sequences1]
opcode_sequences2 = [sequence.split() for sequence in opcode_sequences2]
opcode_sequences3 = [sequence.split() for sequence in opcode_sequences3]

# Combine all opcode sequences into one list
combined_opcode_sequences = opcode_sequences1 + opcode_sequences2 + opcode_sequences3

# Create a LabelEncoder to encode the opcodes
label_encoder = LabelEncoder()

# Flatten the list of lists to create a single list of all opcodes
all_opcodes = [opcode for sequence in combined_opcode_sequences for opcode in sequence]
label_encoder.fit(all_opcodes)

# Encode each sequence using the label encoder
encoded_opcode_sequences = [label_encoder.transform(sequence) for sequence in combined_opcode_sequences]

# Display a sample of the encoded sequences
print("Encoded Opcode Sequences Sample:", encoded_opcode_sequences[:3])

'''
The code on lines 55 through 73 concatenates all encoded opcode sequences into a single 
NumPy array for easier manipulation during model training. It also 
calculates the length of each original sequence, storing these lengths 
in a list to help the Hidden Markov Model (HMM) understand the 
structure of the data. Finally, it prints a sample of the concatenated 
sequences and the lengths of the first ten sequences for verification 
and debugging purposes.
'''

# Concatenate all encoded sequences into a single array
concatenated_sequences = np.concatenate(encoded_opcode_sequences)

# Store the lengths of each sequence
sequence_lengths = [len(sequence) for sequence in encoded_opcode_sequences]

# Display concatenated sequences and their lengths
print("Concatenated Sequences:", concatenated_sequences[:20])
print("Sequence Lengths:", sequence_lengths[:10])

'''
The code on lines 75 through 102 initializes a Hidden Markov Model (HMM) with two hidden states, which 
represent different classifications (e.g., malware and legit). The 
model is configured to iterate 100 times during training for optimal 
parameter fitting. Initial start probabilities are set to equal 
values, indicating an equal likelihood of starting in either 
state, while the transition matrix defines the probabilities of 
moving from one state to another, providing a foundational 
structure for the HMM to learn from the data.
'''

# Define number of hidden states for HMM (e.g., 2 for malware and legit)
n_components = 2

# Display a message about MultinomialHMM module warnings
print(f"\n\nNote: The warnings regarding the changes in MultinomialHMM reflect") 
print(f"updates to the library's implementation. you can safely ignore these")
print(f"warnings, as the code will function correctly despite them.\n\n")

# Initialize HMM with specified parameters
model = hmm.MultinomialHMM(n_components=n_components, n_iter=100, random_state=42)

# Set initial start probabilities and transition matrix
model.startprob_ = np.array([0.5, 0.5])  # Equal probability for both states initially
model.transmat_ = np.array([
    [0.7, 0.3],  # Transition probabilities from state 0
    [0.3, 0.7]   # Transition probabilities from state 1
])

'''
The code on lines 104 through 134 trains the Hidden Markov Model (HMM) using the 
encoded opcode sequences, reshaping the data to fit the model's 
input requirements and providing sequence lengths to guide the 
learning process. After training, a function is defined to 
reinitialize any rows in the transition matrix (transmat_) 
that sum to zero, ensuring that the model remains valid for 
future predictions. This function fills such rows with equal 
probabilities, preventing issues in the model's state transitions, 
and the reinitialized transition matrix is then updated in the model.
'''

# Display a message about startprob_ and transmat_ probability parameters
print(f"\n\nNote: The warnings about startprob_ and transmat_ being overwritten")
print(f"are expected due to the initialization process in MultinomialHMM.") 
print(f"Additionally, zero-sum rows in transmat_ indicate that no transitions") 
print(f"were observed for those states, which is a common occurrence in HMMs") 
print(f"and does not affect the model's functionality.\n\n")

# Train HMM model on encoded opcode sequences
model.fit(concatenated_sequences.reshape(-1, 1), sequence_lengths)

# Reinitialize transmat_ for rows that sum to zero
def reinitialize_transmat(transmat, epsilon=1e-5):
    for i in range(transmat.shape[0]):
        if transmat[i].sum() == 0:
            transmat[i] = np.full(transmat.shape[1], 1.0 / transmat.shape[1])
    return transmat

# Apply smoothing and reinitialize zero-sum rows in transmat_
model.transmat_ = reinitialize_transmat(model.transmat_)

'''
The code on lines 136 through 164 snippet ensures the integrity of the model's initial state 
probabilities (`startprob_`) by checking for any NaN values. 
If found, it reinitializes them to equal probabilities, ensuring a valid 
starting point for the HMM. It then verifies that the sum of the initial 
probabilities equals 1, raising an error if it does not. Lastly, a function 
is defined to check the validity of the transition matrix (`transmat_`) after 
model training, confirming that the model parameters are correctly set up 
for subsequent predictions.
'''

# Check and reinitialize startprob_ if it contains NaN
if np.isnan(model.startprob_).any():
    model.startprob_ = np.full(n_components, 1.0 / n_components)

# Verify startprob_ sums to 1
if not np.isclose(model.startprob_.sum(), 1.0):
    raise ValueError(f"Error: startprob_ must sum to 1 (got {model.startprob_.sum()})")

# Check if the transition matrix is valid
def check_transmat(model):
    try:
        model._check()
        print("Transition matrix is valid.")
    except ValueError as e:
        print(f"Error: {e}")

# After training the model
check_transmat(model)

'''
This code on lines 166 through 194 defines a function to classify a given sequence of opcodes (instructions) as either "Malware" or "Legit" based on a trained Hidden Markov Model (HMM). It first encodes the input opcode sequence using the same LabelEncoder that was used during training, then reshapes the encoded data to fit the model's expected input format. The function calculates the log likelihood of the sequence using the trained model, which measures how likely the sequence is under the learned model parameters. If the log likelihood is below a specified threshold (in this case, -50), the sequence is classified as "Malware"; otherwise, it is classified as "Legit." The example usage demonstrates this classification process on a sample opcode sequence, printing the result.

In this instance, if the output is "Legit," it indicates that the model determined the provided opcode sequence does not resemble those typical of metamorphic malware, suggesting it is from a legitimate application.
'''

def classify_opcode_sequence(opcode_sequence, trained_model, label_encoder):
    try:
        # Encode the sequence using the same label encoder
        encoded_sequence = label_encoder.transform(opcode_sequence)

        # Reshape to match the model input
        reshaped_sequence = np.array(encoded_sequence).reshape(-1, 1)

        # Compute the log likelihood for this sequence
        log_likelihood = trained_model.score(reshaped_sequence)

        # Based on log likelihood, classify as malware or legit
        if log_likelihood < -50:  # Adjust threshold based on your model's performance
            return "Malware"
        else:
            return "Legit"
    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
new_opcode_sequence = ["mov", "add", "jmp", "push"]  # Example sequence
prediction = classify_opcode_sequence(new_opcode_sequence, model, label_encoder)
print(f"\nPrediction for new sequence: {prediction}")


